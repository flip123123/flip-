# 人工智能基础A

结论：外界评价大多是史，但我觉得其实还是能学到一点东西的。不过作为一门两学分的课，需要花的时间确实太多了。可以学会的嘛，最大的我觉得是学会如何使用ai。

基本lab都可以用ai完成，代码也是只是跑的时间比较久，并没有思考上的难度。这里放一下我的大作业好了：

作业要求在[文件夹](https://github.com/flip123123/flip-/tree/master/docs/ai)中的pic,png中

里面有我的模型代码和我的展示ppt（对 做完了大家都要上去展示的），然后要运行的话，要自己配一下环境，代码里的路径也要自己修改一下。

因为训练样本很少（一共75张照片，你还要分训练集和验证集，我是二八分，所以样本就60），我最后训练出来的模型最后只有70左右的正确率

# 人工智能基础A复习


## 人工智能起源

- 世界上第一台通用电子计算机：ENIAC1946年，宾夕法尼亚大学
- 1950年 图灵的图灵机
- 1951年 马文·明斯基建造第一台神经网络计算机
- 人工智能起源于 **1956.8** （人工智能元年）的达特茅斯会议

## 人工智能主要流派

 - 符号主义：基于符号表示-逻辑推理-搜索的人工智能
	 - 认为：人类认知和思维的基本单元是符号，认知过程是以符号表示基础的一种运算，计算机是一个可用来模拟人的智能行为的物理符号系统。
	 - 核心是构建符号表达知识
	 - 主要成果有：机器定理程序、启发式算法、专家系统等
- 联结主义：基于神经网络-深度学习的人工智能
	- 联结主义是一种理解认知过程的理论框架。试图通过模拟大脑的神经网络来解释人类的认知功能，如记忆、学习、语言处理和模式识别。
	- 目标是实现模拟人脑的结构，产生了神经网络方法
	- 主要成果有：神经网络深度学习，现代人工智能大语言模型
- 行为主义：基于环境交互强化学习的人工智能
	- 构建一个智能体，它关注智能体在环境中的行为，智能体以最大化奖励为目标，与环境不断交互来学习和适应，会根据行为的结果进行调整，主要方法包括：（1）强化学习；（2）Q-learning和策略梯度方法
	- 主要成果有：游戏 AI，机器人控制，自动化驾驶等

## 幸存者偏差

- 幸存者偏差，就是忽略了筛选条件，把经过筛选的结果当成随机结果。而AI的局限绝大多数起源于幸存者偏差现象

## 机器学习

### 定义

- 已知一个数据集Dataset ，其中输入X都存在与其对应的标签Y，通过计算机寻找一个数学模型F，使得对于所有X，Y’=F(X)尽可能的逼近Y,这个寻找过程就叫机器学习。  
- 每一对X、Y又叫样本，总个数叫**样本大小**  
- Y的所有可能取值叫**样本空间**， Dataset是样本空间的一个随机抽样  
- **机器学习是人工智能的子集**

### 基本的机器学习算法

- 线性回归算法 Linear Regression
- 支持向量机算法 (Support Vector Machine,SVM)
- 最近邻居/k-近邻算法 (K-Nearest Neighbors,KNN)
- 逻辑回归算法 Logistic Regression
- 决策树算法 Decision Tree
- k-平均算法 K-Means
- 随机森林算法 Random Forest
- 朴素贝叶斯算法 Naive Bayes
- 降维算法 Dimensional Reduction
- 梯度增强算法 Gradient Boosting

### 过拟合

- 指的是训练集上表现很好，预测时表现很差的现象  
- 如何消除过拟合——提升泛化能力交叉验证
	1. dropout：动态随机切断神经元的连接
	2. 正则化：L1范式（LASSO回归）和L2范式（岭回归）
	3. 不过度训练（实时监控）
	4. 特征选择（难度较大，往往事与愿违，以偏盖全）
	5. 集成学习，即多种不同质模型的组合投票
	6. 核心：数据质量与预处理技术
	7. 交叉验证


### 机器学习的分类

- 监督学习
- 无监督学习
- 强化学习

### 常用损失函数

**一、回归损失函数**

1. **均方误差 (Mean Squared Error, MSE)**
    
    - **公式:**$$MSE = \frac{1}{N} \sum_{i=1}^{N} (T_i - Y_i)^2 $$
        
    - **字母含义:**
        - NN: 数据点的总数
        - TiTi​: 第 ii 个数据点的真实值 (Target)
        - YiYi​: 第 ii 个数据点的预测值 (Prediction)
2. **平均绝对误差 (Mean Absolute Error, MAE)**
    
    - **公式:**$$MAE = \frac{1}{N} \sum_{i=1}^{N} |T_i - Y_i| $$
    - **字母含义:**
        - NN: 数据点的总数
        - TiTi​: 第 ii 个数据点的真实值 (Target)
        - YiYi​: 第 ii 个数据点的预测值 (Prediction)
3. **平均绝对百分比误差 (Mean Absolute Percentage Error, Map)**
    
    - **公式:**$$Map = \frac{1}{N} \sum_{i=1}^{N} \left| \frac{Y_i - T_i}{Y_i} \right| \times 100 $$
        
    - **字母含义:**
        - NN: 数据点的总数
        - TiTi​: 第 ii 个数据点的真实值 (Target)
        - YiYi​: 第 ii 个数据点的预测值 (Prediction)
4. **均方根对数误差 (Mean Squared Logarithmic Error, MSLE)**
    
    - **公式:**$$ MSLE = \frac{1}{N} \sum_{i=1}^{N} [\log(T_i + 1) - \log(Y_i + 1)]^2 $$

    - **字母含义:**
        - NN: 数据点的总数
        - TiTi​: 第 ii 个数据点的真实值 (Target)
        - YiYi​: 第 ii 个数据点的预测值 (Prediction)
        - log⁡log: 自然对数 (通常)

**二、分类损失函数**

1. **二进制定制交叉熵 (Binary Cross-entropy)**
    
    - **适用场景:** 用于二分类问题，通常搭配 Sigmoid 激活函数。
    - **公式:**$$\text{Loss} = -\frac{1}{N} \sum_{i=1}^{N} [Y_i \times \log T_i + (1 - Y_i) \times (1 - T_i)] $$
        
    - **字母含义:**
        - NN: 数据点的总数
        - YiYi​: 第 ii 个数据点的真实标签 (通常是 0 或 1)
        - TiTi​: 第 ii 个数据点的预测概率 (介于 0 和 1 之间)
        - log⁡log: 自然对数 (通常)
2. **多分类交叉熵 (Categorical Cross-entropy)**
    
    - **适用场景:** 用于多分类问题，通常搭配 Softmax 激活函数。
    - **公式:**$$\text{Loss} = -\frac{1}{N} \sum_{i=1}^{N} Y_i \times \log T_i $$

    - **字母含义:**
        - NN: 数据点的总数
        - YiYi​: 第 ii 个数据点的真实标签 (通常是独热编码，例如 [0, 1, 0])
        - TiTi​: 第 ii 个数据点的预测概率分布 (例如 [0.1, 0.7, 0.2])
        - log⁡log: 自然对数 (通常)

### 聚类

- 聚类是指把特征相似的样本聚成一组，同组成员有相似的属性。即物以类聚，人以群分
- 聚类是一种无监督学习，算法的核心思想是通过计算数据点之间的相似度或距离（欧式距离），将相似的数据点聚集在一起形成簇

### k-means 算法

1. 随机初始化聚类质心
2. 计算每个数据到质心的距离，并归类到最近的距离
3. 重新计算每类的质心
4. 重复循环第二、三步，直到满足条件  
[K-means 演示](http://alekseynp.com/viz/k-means.html)

### 降维

- 原理：通过建立一个新的坐标系，将多个维度的数据投影到该坐标系，实现数据的降维，而这个坐标系的轴方向就是主成分  
- 目标：在新坐标系中实现数据的波动最大（方差最大）

## 深度学习

### 原理

- 深度学习（Deep Learning，DL）是机器学习的一个子集，它使用多层人工神经网络来精准完成诸如物体检测、语音识别、语言翻译等任务。其基本思想是通过构建多层网络，对目标进行多层表示，以期通过多层的高层次特征来表示数据的抽象语义信息，从而获得更好的特征鲁棒性。

### 特征

- 多层网络结构：由多层神经元组成，包括输入层、隐藏层和输出层。这些层之间通过权重和偏置进行连接，形成复杂的网络结构
- 自动特征提取：自动从原始数据中提取有用的特征，无需人工设计特征工程
- 非线性激活函数：深度学习模型中通常使用非线性激活函数（如ReLU、sigmoid等），使得模型能够学习复杂的非线性关系
- 大规模数据处理能力：借助现代计算技术和大数据资源，深度学习模型能够处理大规模数据集，提高模型的准确性和泛化能力

### 感知机

- 输入 X 可以是一个任意 n 维的向量，W 是 X 的权重向量，网络∑=WX^T+b 完成线性变换  
- 再通过激活函数 f，完成非线性变化

### 多层感知机

- 定义：多层感知机是罗森布拉特标准感知器的扩展。如果以神经元来计算层数，一个多层感知器至少包含三层：一个输入层，一个隐藏层和一个输出层
- 数据通过输入层进入网络，乘以连接的权重后输入到隐藏层节点。隐藏层节点将这些加权后的输入求和并经过一个非线性变换（称为激活函数）后送往输出层。
- 常见的激活函数：
	- Sigmoid：1/（1+e^(-x)）
	- ReLU: max (0, x)

### 误差反向传播算法-BP

- 反向传播算法是一种常见的人工神经网络学习算法，特别适用于多层前馈神经网络的训练。该算法由学习过程中的信号正向传播与误差的反向传播两个过程组成

### 梯度下降

- 定义：以最快的速度找到函数局部最小值的优化算法
- 如何应用：设计损失函数，沿着损失函数梯度的反方向更新网络参数

#### 优化器

- 随机梯度下降法 （Stochastic Gradient Descent, SGD）一次计算全部样本的误差之和一次只计算一个（批）样本‘
	- 小批量梯度下降法（Mini-batch SGD）；动量梯度下降法（Momentum SGD）-梯度移动平均 
代码实现： 
`optimizer = optim.SGD(model.parameters(), lr=0.01)`  
`optimizer = optim.SGD(model.parameters, momentum=0.9) `
- 自适应梯度算法 Adaptive Gradient,，AdaGrad根据权重的梯度自适应调整学习率，分母增加历史梯度累积值
	- 自适应平方根梯度法（RMSProp）；自适应矩估计法（Adam） 目前最常用  
代码实现：  
`optimizer =optim.Adagrad(model.parameters(),lr=0.01,weight_decay=1e-4, eps=1e-10)`  
`optimizer = optim.Adam(model.parameters(),lr=0.01,betas=(0.9, 0.999),eps=1e-8,weight_decay=0) `

### 卷积运算（CNN）

卷积运算就是通过设计一系列大小适中的卷积核（感受野），对数字图像的各个通道分量（二维矩阵）进行卷积，提取特征值的过程。常用的卷积核大小为3×3、5×5、7×7。  
卷积运算中的三个重要参数：

1. 卷积核形状：正方形的 3×3，5×5，7×7
2. 步长：决定运算的速度
3. 卷积核数量：决定每一层能提取的特征数

### 池化

池化也称下采样，其作用就是缩小特征图的尺寸，减少计算量。池化的原理是可以用某一图像区域子块的统计信息包含了该子块全局信息。池化操作主要有最大池化、平均池化、随机池化、L2范数池化、K-max池化和全局平均池化等。CNN常用2×2区域进行池化。

### RNN

循环神经网络（Recurrent Neural Network，RNN）：是一种特殊类型的反馈神经网络，专门用于处理序列数据。RNN的核心思想在于其循环结构，使得网络能够捕捉和利用序列中的顺序依赖性信息。RNN的基本单元是一个具有循环连接的神经网络层。这个循环连接允许网络在处理每个时间步的数据时，能够利用之前时间步的信息。具体来说，RNN在每个时间步都会接收一个输入，并产生一个输出，同时其内部状态（隐藏状态）会被更新并传递到下一个时间步。

### RNN 计算

直接去ppt上看例题吧，懒得弄链接转换了

## 自然语言处理（NLP）

### NLP 任务：

1. 文本转换
2. 语音识别
3. 文本生成
4. 文本分析

### 分词

Token：令牌，可以是一个字，也可以是一个词，或者是一个字母，甚至是一个字节，要看具体的情况。本质上，一个“Token”就是通过分词技术（工具）将一句话分割成的最小单位，是一个特定的自然语言处理模型能处理的最基本元素，至小有内，至大无外。

### 词向量和词嵌入

词向量就是词的特征分布，是NLP模型层与层之间进行信息传递的数据形式  
词向量和词嵌入是指的同一个东西，区别在于词向量是指数字编码技术，词嵌入是指NLP网络之间的数据存在形式

### Transformer
**特点** ：
1. 注意力机制
2. 自回归生成机制
3. 整体机制

具体原理可以看看这篇文章[一文了解Transformer全貌（图解Transformer）](https://www.zhihu.com/tardis/zm/art/600773858)
### LLM

是指特定用于执行 NLP 任务的大模型，大的含义：

1. 训练数据大：以万亿为单位数据来源领域广大，涵盖人类现有研究的所有门类
2. 参数规模大：以千亿为单位，以Transformer结构为基础
3. 耗资巨大：以百万美元为单位

涌现能力：当一种系统在复杂性增加到某一临界点时，会出现其子系统或较小规模版本中未曾存在的行为或特性

### 各种AI

### GAI

GAI（Generative Artificial Intelligence，生成式人工智能），是特指能生成全新内容的AI，其生成的内容就是前面所提的AIGC，AIGC侧重内容生成的来源，而GAI侧重AI系统的功能特点。

注意问题：

1. 自回归生成技术，带有随机性
2. 不是搜索引擎

### AGI

AGI（Artificial General Intelligence，通用人工智能），是指机器能够完成人类能够完成的任何智力任务的能力。它旨在实现一般的认知能力，能够适应任何情况或目标，是人工智能研究的最终目标之一。AGI能够执行各种复杂的任务，包括学习、计划、解决问题、抽象思维、理解复杂理念等。即通五明。

### GPT 和 ChatGPT

- GPT（Generative Pre-trained Transformer），是一个基于Transformer结构的预训练模型
- ChatGPT（Chat Generative Pre-trained Transformer），是一个采用GPT架构的聊天机器人产品

### AI 绘画

里程碑事件

1. 20世纪70年代，AARON，通过机械臂进行作画，控制机械臂的是一套计算机程序算法，艺术家哈罗德·科恩
2. 2012年，谷歌的吴恩达和Jeff Dean使用卷积神经网络（CNN），基于大量猫脸图片训练出了一个能够生成模糊猫脸的模型
3. 2014年，生成式对抗网络（GAN），加拿大蒙特利尔大学伊恩·古德费罗
4. 2015年，Deep Dream（深梦），谷歌
5. 2021年，DALL-E，OpenAI
	1. DALL-E1：GPT-3（Transformer） + VAE（自分编码器）
	2. DALL-E2：CLIP（视觉语言预训练模型） + Diffusion（扩散模型）
	3. DALL-E3：CLIP + VAE + Diffusion（扩散模型）

### CLIP 和扩散模型

- **CLIP（视觉语言预训练模型）**：文本信息通过文本编码器进行编码，图像信息通过图像编码器进行编码，二者的编码信息存入多模态的隐空间中。所谓的隐空间就是数据的一种表示和存储方式，即将现实世界的实体（如本文中的图像、文本）编码为计算机算法可运算的数据格式。文本编码器和图像编码器的参数经过模型训练获得最优值，以实现文本与图像的匹配
- **前向过程（扩散过程）**：原始图像G0经过不断的加入高斯噪声生成模糊图像（即打马赛克），经过n步，最终生成一副不再扩散的稳定图像Gn。
- **反向过程（去噪过程）**：从图像Mn=Gn出发，通过带参数的模型U-Net一步步实现去噪过程恢复图像的原始状态，经过n步，最终获得去噪后的图像M0。这个模型U-Net就是扩散模型

### 多模态大语言模型MLLM

多模态大语言模型（Multimodal Large Language Model，MLLM）：够感知不同模态的输入，如图片、文字、声音等，并根据人类给出的指令，以自回归的方式学习上下文并生成回答。MLLM使用了自然语言处理、计算机视觉和语音识别等技术，并  
将它们整合到一个系统中，从而能够更加准确地理解人类的语言和情感。

注：模态：任意形式的信息都可以视为一个模态，如文本、图像、语音、光波、气味等

